---
title: "Final Project - 36-462: Data Mining"
author: "Sivan Mehta, Mary St. John, and Graceanne Wong"
date: "5/12/2017"
output:
  pdf_document:
    fig_height: 4
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
# import data sets and any libraries we might need
source("./analysis/clean-data.R")
source("./analysis/timeseries.R")
library(ggplot2)
library(dplyr)
```

# Introduction

The dataset at hand concerns flights to and from the Pittsburgh International Airport (PIT). Each observation is a single flight either entering or leaving Pittsburgh. While the predictive focus will be on departing flights, we will also look at flights overall to get a general structural overview of the dataset.

Of the 57 variables provided, they can be split into a few primary groups. Time Period information concerns the date of the flight. Airline information pertains to the specific airline and carrier of the flight. Origin information concerns origin airport details, and Destination information contains similar details. Departure and Arrival performance information contains metrics on the delays, taxiing, etc. Finally, there are general delay metrics, detailing the breakdown of what contributed to the overall delay, if any.

To predict flights, we will be working with three data sets with flight data from the Pittsburgh airport. Our training dataset is all the arrivals and departures from 2015. Our test data is arrivals and departures up to a random timepoint in the day. After that time point, we have unlabeled data on which we will form our guesses.

In this report, we will focus on *departing* flights and predict whether or not the flight will be delayed. We will first take a general unsupervised learning approach, attempting to ascertain any underlying structure to the dataset. Then we will move towards supervised analysis, attempting to actually predict whether or not a flight will be delayed.

# EDA and unsupervised analysis

First, we'll examine delays on each day, in order to reduce *some* of the noise, we'll also plot some moving averages of delays.

```{r, warning = FALSE}
by_date <- group_by(train, FL_DATE)
delays_per_day <- summarize(by_date,
                            N_FLIGHTS = n(),
                            DEP_DELAY_TOT = sum(DEP_DEL15, na.rm = TRUE),
                            ARR_DELAY_TOT = sum(ARR_DEL15, na.rm = TRUE))


delays_per_day$FL_DATE <- as.Date(delays_per_day$FL_DATE)
delays_per_day$DEP_AVG_7 <-get_moving_averages(delays_per_day$DEP_DELAY_TOT, 7)
delays_per_day$DEP_AVG_30 <-get_moving_averages(delays_per_day$DEP_DELAY_TOT, 30)

# two week departures delays moving averages
ggplot(delays_per_day) +
  scale_x_date() +
  geom_line(aes(x = FL_DATE, y = DEP_DELAY_TOT), alpha = .1) +
  geom_line(aes(x = FL_DATE, y = DEP_AVG_7), color = "red", alpha = .5) +
  geom_line(aes(x = FL_DATE, y = DEP_AVG_30), color = "blue", size = 1) +
  labs(x = "Date of Flight", y = "Number of Delays",
       title = "Delays per day, with weekly average in red, monthly average in blue")
```

Here we can see some clear seasonal effects surrounding delays when looking at the moving average. While there is *a lot* of volatility as seen in the red weekly average, we can relatively smooth this with the blue monthly average. This tells us we can gain some insight with the date information. Next, we can try clustering to see if they flights break down into meaningful groups. This perhaps could give us some insight on the general types of flights we may see.

```{r, warning = FALSE}
library(protoclust)
sample.data <- sample_n(vis, nrow(vis) * .1)
flights.dist <- dist(sample.data)

tree.com <- hclust(flights.dist, method = "complete")
tree.proto <- protoclust(flights.dist)

par(mfrow = c(1, 2))

plot(tree.com, main = "Complete Linkage")
plot(tree.proto, main = "Prototype Clustering")

assignments.com <- cutree(tree.com, 2)
assignments.proto <- protocut(tree.proto, 2)

predictions.com <- assignments.com - 1
right.com <- length(which(predictions.com == sample.data$DEP_DEL15)) / nrow(sample.data)
misclass.com <- right.com

predictions.proto <- assignments.proto$cl - 1
right.proto <- length(which(predictions.proto == sample.data$DEP_DEL15)) / nrow(sample.data)
misclass.proto <- right.proto

late <- train[which(train$DEP_DEL15 > 0),]
on.time <- train[which(train$DEP_DEL15 == 0),]
base.rate <- nrow(late) / (nrow(late) + nrow(on.time))
```

Generally, these are doing pretty poorly, garnering misclassification rates of nearly 50%, **much** worse than the base rate of `r round(base.rate*100, 2)`% When using the assigned clusters, at almost any level, as a classification.

# Supervised analysis

Before we began our supervised analysis, we created new features in the data. After looking at the raw data given to us about the flights we need guess on, we determined that there was not much information there. We built features based on previous events in the day, mostly focusing on the percentage of flights that have been affected by events that day. For each day, we calculated what percent of flights were delayed before the plane was scheduled to leave, and we did the same for weather delay, national air system delay, and delays on arrival flights. Because percent of flights delayed at a given point is not very indiciative of the rate of delays at a certain time, we created a feature based on the derivative of these ratios as a function of time. In calculating this, we kept the denominator of the ratio fixed and equal to the total number of flights scheduled for that day. 

We had many approaches to making a prediction model. We began with a random forest on all predictors to determine the most important ones by using a variable importance plot. We then put these predictors into linear models, additive models, logistic regression models, splines like k nearest neighbor and kernel smoothers. These models proved to be at or near the base rate of classifying delayed and not delayed flights. 

We made predictions using a mixture model. The mixture model was trained on an indicator variable which was based on current truth, which is not available in the guess data set which is why we predicted that all flights are not delayed.

- How did you make your predictions? Describe this process in detail.
- You can use any of the classification techniques that we learned in the first half of the course, or any other techniques as long as they are adequately described.
- What predictor variables did you include? How did you engineer features from the data? What technique did you use for prediction, and why did you choose it?
- If there were tuning parameters, how did you pick their values? Can you explain anything about the nature of the relationship between the predictors in your model and the predictions themselves?
- Explain what all of the indicator variables
    - how were they formed
    - why we thought they would be useful
- Explain each methodology's approach
    - linear model / glm / naive bayes
    - random forest
    - mixture model
    - how we used each of the variables
- why do we think we failed?
- Desired changes to problem that would have helped us
