---
title: "Analysis Random Forest"
author: "Mary St John"
date: "April 16, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(glmnet)
```

### Loading the data and cleaning it
```{r}
source("analysis/clean-data.R")
source("./features/ordered-data-set.R")
source("./features/NAS-delay.R")
source("./features/NAS-delay.R")
base_rate = mean(train$DEP_DEL15)
x.train = train[,-(which(colnames(train) == "DEP_DEL15"))]
y.train = train[,which(colnames(train) == "DEP_DEL15")]
```

### Linear Model
```{r}
fit_lm = lm(DEP_DEL15 ~ ., data = train)
predslm = predict(fit_lm, newdata = test)
errTest_lm = mean((predslm - test[,1])^2) #.1320 < base rate!
```

# Lasso
```{r}
library(mgcv)
fit_lasso = cv.glmnet(as.matrix(x.train), y.train)

predsLasso_train = predict(fit_lasso, newx = as.matrix(x.train), type = "response", s = fit_lasso$lambda.1se)
predsLasso_test = predict(fit_lasso, newx = as.matrix(x.test), type = "response", s = fit_lasso$lambda.1se) 
errTrain_lasso = mean((predsLasso_train - y.train)^2) #.1346
errTest_lasso = mean((predsLasso_test - y.test)^2) # .1331

coeffs = which(coefficients(fit_lasso) != 0)[-1] - 1 #There's only one variable in this model
```

# GLM
```{r}
fit_glm = glm(DEP_DEL15 ~ ., data = train, family='binomial')
guess_test = predict(fit_glm, newdata = test, type='response')
errTrain_glm = mean((fitted(fit_glm) - y.train)^2) # .1329
errTest_glm = mean((guess_test - y.test)^2) # .1319
```

#Fit a Random Forest
```{r}
library(randomForest)
natrain = na.omit(train)
natrain = natrain[,-which(colnames = )]
rf = randomForest(as.factor(DEP_DEL15)~.,data = natrain, importance=TRUE)
errTrain.rf = mean(y.train == rf$predicted) # .1725 Not good
if (errTrain.rf > .5){errTrain.rf = 1 - errTrain.rf}
testPreds = predict(rf, newdata = test, type = "response")
errTest.rf = mean(y.test == testPreds)
if (errTest.rf > .5){errTest.rf = 1 - errTest.rf} # .1686 Not good

# Variable importance plot
varImpPlot(rf.out, main = "Variable importance plots")
```

# LDA
```{r}
library(MASS)
fit_lda = lda(x.train,y.train)
yhat_lda = predict(fit_lda,newdata = x.test)$class
misclass_rate_lda = mean(yhat_lda!=y.test) #.1652 Not good

#Build the transformed coordinates for the training and test data, in case you need them
z_train = x.train%*%fit_lda$scaling
z_test = x.test%*%fit_lda$scaling

plot(z_train[,1],z_train[,2], pch=as.character(y.train), col = y.train+1)
```

# SVM
```{r}
fit_svm_linear = tune(svm,y~.,data=subsample,scale=FALSE, kernel="linear", 
                      ranges=list(cost=c(4.05, 4.1, 4.15)))
```

# Naive Bayes
```{r}
library("e1071")
y.train = as.factor(y.train)
fit_naive_bayes = naiveBayes(x.train, y.train)
guess_test = predict(fit_naive_bayes, newdata = x.test)
errTrain_naive_bayes = mean((fitted(fit_naive_bayes) - y.train)^2) # .1329
errTest_naive_bayes = mean((as.integer(guess_test) - y.test)^2) # .1319
```

# Boosted Trees
```{r}

```

